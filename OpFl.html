<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="">
	<meta name="author"      content="Marc van Zyl">
	
	<title>Marc van Zyl's website</title>

	<link rel="shortcut icon" href="assets/images/gt.png">
	
	<!-- Bootstrap -->
	<link href="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.no-icons.min.css" rel="stylesheet">
	<!-- Icon font -->
	<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
	<!-- Fonts -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Alice|Open+Sans:400,300,700">
	<!-- Custom styles -->
	<link rel="stylesheet" href="assets/css/styles.css">

	<!--[if lt IE 9]> <script src="assets/js/html5shiv.js"></script> <![endif]-->
</head>
<body>

<header id="header">
	<div id="head" class="parallax" parallax-speed="1">
		<h1 id="logo" class="text-center">
			<span class="title">Marc van Zyl</span>
			<span class="tagline">Inventor, leader, and software engineer<br>
				<a href="">marcfvanzyl@gmail.com</a></span>
		</h1>
	</div>

	<nav class="navbar navbar-default navbar-sticky">
		<div class="container-fluid">
			
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
			</div>
			
			<div class="navbar-collapse collapse">
				
				<ul class="nav navbar-nav">
					<li><a href="index.html">Home</a></li>
					<li class="active"><a href="about.html">About</a></li>
					<li class="dropdown">
						<a href="#" class="dropdown-toggle" data-toggle="dropdown">Projects <b class="caret"></b></a>
						<ul class="dropdown-menu">
							<li><a href="athegia.html">Athegia</a></li>
							<li><a href="OpFl.html">Optical Flow VS</a></li>
							<li><a href="drone.html">Autonomous Precision Landing</a></li>
							
						</ul>
					</li>
					
					<li><a href="resume.html">Resume</a></li>
				</ul>
			
			</div><!--/.nav-collapse -->			
		</div>	
	</nav>
</header>

<main id="main">

	<div class="container">

		<div class="row topspace">
			
			<!-- Sidebar -->
			<aside class="col-md-4 sidebar sidebar-left" style="margin-left: 0; text-align: left;">
				<div class="row widget">
					<div class="col-xs-12">
						<h4><a href="https://github.com/Marcsprk43/OpticalFlow">GitHub</a></h4>
					</div>
				</div>

				<div class="row widget">
					<div class="col-xs-12">
						<h4>Competitions and awards</h4>
						<p><b>Science and Engineering Fair</b>
						<ul><li>2021 RESA Regional: 1st Place</li>
							<li>2021 RESA Regional: Regeneron ISEF Award</li>
							<li>2021 RESA Regional: Office of Naval Research Naval Science Award</li>
							<li>2021 RESA Regional: United States Air Force Award</li>
							<li>2021 RESA Regional: Mu Alpha Theta Award</li>
							<li>2021 Fayette County: 1st Place</li>
							<li>2020 Georgia State: 1st Honors</li>
							<li>2020 RESA Regional: 1st Place & U.S. Metric Association Award</li>
							<li>2020 Fayette County: 1st Place</li>
							</ul></p>
					</div>
				</div>

			</aside>			<!-- /Sidebar -->

			<!-- Article main content -->
			<article class="col-sm-8 maincontent">
				<header class="page-header">
					<h1 class="page-title">We based our vision systems on our own two eyes. But is that really how we see?</h1>
				</header>
				<p>The inability of robots to accurately map 3D environments is one of the biggest hurdles to greater adoption of autonomous robots in our chaotic, unstructured environments. The most common 3D depth mapping systems currently used are stereo vision systems and LiDAR. Stereo systems, which require 2 cameras, are computationally costly and are prone to errors, while LiDAR systems are very expensive, have slow update cycles, and are cumbersome. I had a hypothesis that we see primarily through movement of our head, in a process called optical flow, rather than with a stereo system design. The purpose of my project was to explore the feasibility of a depth mapping system for robots based on dense optical flow as implemented in most GPU chips by extending the Gunnar Farneback algorithm. Optical flow algorithms measure the speed of movement of objects in sensor space given a stationary camera. I set out to determine whether this technology can be applied to measuring the distance to stationary objects in real-world space if the cameraâ€™s velocity in real-world coordinates is known, as is the case in most ground-based robots.</p>
				<h3>Design Requirements</h3>
				<p>To perform an analysis, I needed to create two vision systems and make an environment to test them on.</p>
				<ol>
					<li>The control setup needed to:
						<ul><li>Be a stereo vision system, with two cameras</li>
							<li>Use the same cameras as my experimental setup</li>
							<li>Be precise enough on the camera spacing to be as accurate as possible</li></ul>
					</li>
					<li>The experimental setup needed to:
						<ul><li>Use a single camera</li>
							<li>Smoothly and accurately move from left to right and rotate</li>
							<li>Record high-quality footage</li>
					</li></ol>
				</ol>
				<h3>The testing setup</h3>
				<p>I created my own stereo and optical flow setup for this experiment</p>
				<p><b>Control (Stereo) Setup</b><br><img src="assets/images/stereo.png" alt="" width="70%" height="70%"></p>
				<blockquote>The stereo setup was easy - two cameras, a fixed distance apart, controlled by a Raspberry Pi and a simple 3D printed stand for the cameras.</blockquote>
				<p><b>Experimental (Optical Flow) Setup</b><br><img src="assets/images/of.png" alt="" width="70%" height="70%"></p>
				<blockquote>The experimental setup was not so easy. I needed to use a spare 3D printer control board to control 2 stepper motors - one for movement and one for rotation. I also used a more powerful controller, the Jetson Nano, in order to capture uncompressed video.</blockquote>
				<p><b>Testing Setup</b><br><img src="assets/images/scene.png" alt="" width="70%" height="70%"></p>
				<blockquote>I set up my targets (images with many lines to track) between aruco markers, which are easy for a computer to identify and find the location of. Each setup is then tasked with finding the distance to each target over multiple trials.</blockquote>
				
				<b>Underlying theory</b><br>
				The stereo system, because the pictures are always a set distance apart at a single point in time, is simplistic in calculation. The distance shift in pixels between two cameras is multiplied by a constant determined by the focal length and distance between cameras<br>
				<img src="assets/images/stereo_theory.png" width="70%" height="70%"><br>
				Because optical flow wasn't meant to be done with a moving camera (the scene should be the one moving) I had to come up my own distance calculations as shwon in my diagram below. This systems works with linear (sideways, but easily adaptable for up and down too) and rotational motion but as of now is unable to handle forward backwards movement to a target<br>
				<img src="assets/images/of_theory.png" width="70%" height="70%"></p>

				<h3>Results</h3>
				<p><img src="assets/images/OF results.png" width="70%" height="70%"></p>
				<p><b>Findings: </b><br>
					For the special case of linear camera motion, the experiment confirms that Optical Flow algorithms can achieve comparable accuracies to traditional Stereo vision systems with much lower variance in measurements. While the overall accuracy of Stereo vision was greater at the very close distances, Optical Flow was more accurate at the longer distances. This is probably due to the Optical Flow algorithm using multiple images from the video stream to update measurements over time. This points to some interesting possibilities for robotic vision in the future. Dense optical flow is particularly interesting due to its ability to map a full scene and refine its accuracy over time.
				</p>
				
				<h3>Learnings</h3>
				<p><ul>
					<li>How to use OpenCV and vision systems in general</li>
					<li>Data streams (in video gathering)</li>
					<li>Deep data analysis with Pandas and MatPlotLib</li>
					<li>How to write gcode commands</li>
					<li>How to use trig in 3 dimentions to pull answers form complex systems</li>
					</ul></p>

			</article>
			<!-- /Article -->
			

		</div>
	</div>	<!-- /container -->
	
</main>

<footer id="footer" class="topspace">
	<div class="container">
		<div class="row">
			<div class="col-md-3 widget">
				<h3 class="widget-title">Contact</h3>
				<div class="widget-body">
					<p>+1 678 554 8327<br>
						<a href="mailto:#">marcfvanzyl@gmail.com</a><br>
						<br>
						Peachtree City, GA
					</p>	
				</div>
			</div>

			<div class="col-md-3 widget">
				<h3 class="widget-title">Follow me</h3>
				<div class="widget-body">
					<p class="follow-me-icons">
						<a href=""><i class="fa fa-github fa-2"></i></a>
						<a href=""><i class="fa fa-facebook fa-2"></i></a>
					</p>
				</div>
			</div>


		</div> <!-- /row of widgets -->
	</div>
</footer>

<footer id="underfooter">
	<div class="container">
		<div class="row">
			
			<div class="col-md-6 widget">
				<div class="widget-body">
					<p class="text-right">
						Copyright &copy; 2023, Marc van Zyl<br> 
						 </p>
				</div>
			</div>

		</div> <!-- /row of widgets -->
	</div>
</footer>



<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>
<script src="assets/js/template.js"></script>
</body>
</html>
